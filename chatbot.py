from fastapi import FastAPI, UploadFile, File, Form
from pydantic import BaseModel
from langchain_community.llms import HuggingFaceEndpoint
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFaceHub
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyMuPDFLoader
from PyPDF2 import PdfFileWriter, PdfFileReader
import io
import base64
import os

# Initialize FastAPI instance
app = FastAPI()

# Hugging Face API token for authentication
api_token = "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"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = api_token

# Initialize Hugging Face endpoint for language model interaction
llm = HuggingFaceEndpoint(repo_id="google/gemma-1.1-2b-it",
                          max_new_tokens=1000,
                          huggingfacehub_api_token=api_token)

# List to store chat messages
messages = []

# Pydantic models for request bodies
class UserInput(BaseModel):
    input: str

class PDFQuery(BaseModel):
    pdf: str
    input: str

class Path(BaseModel):
    path: str

# Utility function to save byte data as a PDF file
def save_bytes_as_pdf(byte_data, output_path):
    """
    Save byte data as a PDF file.

    Args:
    - byte_data (bytes): Byte data representing the PDF content.
    - output_path (str): Path where the PDF file should be saved.
    """
    pdf_reader = PdfFileReader(io.BytesIO(byte_data))
    pdf_writer = PdfFileWriter()
    for page_num in range(pdf_reader.getNumPages()):
        page = pdf_reader.getPage(page_num)
        pdf_writer.addPage(page)
    with open(output_path, 'wb') as output_pdf:
        pdf_writer.write(output_pdf)

# Function to handle chatbot responses
def chatbot_response(user_input, messages, llm):
    """
    Generate a response from the chatbot.

    Args:
    - user_input (str): User input to the chatbot.
    - messages (list): List to store chat history.
    - llm (HuggingFaceEndpoint): HuggingFaceEndpoint instance for generating responses.

    Returns:
    - str: Response generated by the chatbot.
    """
    messages.append({"role": "user", "content": user_input})
    prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
    output_text = llm.invoke(prompt).strip()
    messages.append({"role": "bot", "content": output_text})
    return output_text

# Function to load and process a PDF document
def load_doc(pdf_doc_path):
    """
    Load and process a PDF document for querying.

    Args:
    - pdf_doc_path (str): Path to the PDF document.

    Returns:
    - RetrievalQA: Instance of RetrievalQA for querying the loaded document.
    """
    loader = PyMuPDFLoader(pdf_doc_path)
    documents = loader.load()
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    text = text_splitter.split_documents(documents)
    db = Chroma.from_documents(text, embedding)
    llm = HuggingFaceHub(repo_id="OpenAssistant/oasst-sft-1-pythia-12b", model_kwargs={"temperature": 1.0, "max_length": 256})
    chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=db.as_retriever())
    return chain

# Function to answer queries based on a loaded document
def answer_query(query, chain):
    """
    Answer a query based on a loaded document.

    Args:
    - query (str): Query to be answered.
    - chain (RetrievalQA): Instance of RetrievalQA containing the document for querying.

    Returns:
    - str: Answer to the query.
    """
    return chain.run(query)

# Endpoint for chat interaction
@app.post("/chat")
async def chat_endpoint(user_input: UserInput):
    """
    Endpoint for interacting with the chatbot.

    Args:
    - user_input (UserInput): Input from the user.

    Returns:
    - dict: Response from the chatbot.
    """
    response = chatbot_response(user_input.input, messages, llm)
    return {"response": response}

# Endpoint for querying a PDF document
@app.post("/query_pdf/")
async def query_pdf_endpoint(pdf_query: PDFQuery):
    """
    Endpoint for querying a PDF document.

    Args:
    - pdf_query (PDFQuery): PDF and input query.

    Returns:
    - dict: Response to the query.
    """
    path = "uploaded.pdf"
    pdf_bytes = base64.b64decode(pdf_query.pdf)
    with open(path, 'wb') as f:
        f.write(pdf_bytes)
    chain = load_doc(path)
    response = answer_query(pdf_query.input, chain)
    response = response.split("Helpful Answer: ")[-1]
    # print(response)
    return {"response": response}

# Run the FastAPI application
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, port=8000, host="127.0.0.1")
